{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b632df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T23:24:15.036060Z",
     "iopub.status.busy": "2025-03-10T23:24:15.035671Z",
     "iopub.status.idle": "2025-03-11T05:41:18.235451Z",
     "shell.execute_reply": "2025-03-11T05:41:18.233798Z"
    },
    "papermill": {
     "duration": 22623.206887,
     "end_time": "2025-03-11T05:41:18.239049",
     "exception": false,
     "start_time": "2025-03-10T23:24:15.032162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1887 files belonging to 2 classes.\n",
      "Found 402 files belonging to 2 classes.\n",
      "Found 410 files belonging to 2 classes.\n",
      "\n",
      "Training 1/10: (0.001, 64, 0.2, 32, (5, 5), 256, True, 'leaky_relu')...\n",
      "  Early stopping triggered at epoch 4.\n",
      "  New best model saved: saved_models/best_model.keras\n",
      "\n",
      "Training 2/10: (0.001, 64, 0.5, 64, (5, 5), 128, False, 'relu')...\n",
      "  Early stopping triggered at epoch 10.\n",
      "  New best model saved: saved_models/best_model.keras\n",
      "\n",
      "Training 3/10: (0.01, 64, 0.2, 64, (5, 5), 128, False, 'leaky_relu')...\n",
      "  Early stopping triggered at epoch 3.\n",
      "\n",
      "Training 4/10: (0.01, 32, 0.5, 64, (5, 5), 256, False, 'leaky_relu')...\n",
      "  Early stopping triggered at epoch 3.\n",
      "\n",
      "Training 5/10: (0.001, 64, 0.2, 32, (3, 3), 128, False, 'relu')...\n",
      "  Early stopping triggered at epoch 7.\n",
      "  New best model saved: saved_models/best_model.keras\n",
      "\n",
      "Training 6/10: (0.0001, 32, 0.5, 64, (5, 5), 256, True, 'relu')...\n",
      "  Early stopping triggered at epoch 5.\n",
      "\n",
      "Training 7/10: (0.01, 64, 0.5, 64, (5, 5), 128, False, 'relu')...\n",
      "  Early stopping triggered at epoch 3.\n",
      "\n",
      "Training 8/10: (0.001, 64, 0.5, 64, (5, 5), 128, True, 'relu')...\n",
      "  Early stopping triggered at epoch 7.\n",
      "\n",
      "Training 9/10: (0.0001, 64, 0.2, 64, (5, 5), 128, False, 'relu')...\n",
      "  Early stopping triggered at epoch 10.\n",
      "  New best model saved: saved_models/best_model.keras\n",
      "\n",
      "Training 10/10: (0.001, 64, 0.5, 64, (3, 3), 256, True, 'leaky_relu')...\n",
      "  Early stopping triggered at epoch 7.\n",
      "\n",
      "All results saved to: saved_models/all_results.json\n",
      "Best model details saved to: saved_models/best_model_info.json\n",
      "Best model file: saved_models/best_model.keras\n",
      "Best training history file: saved_models/best_history.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Define dataset paths\n",
    "base_dir = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset_2n_version\"\n",
    "train_dir = f\"{base_dir}/train\"\n",
    "val_dir = f\"{base_dir}/val\"\n",
    "test_dir = f\"{base_dir}/test\"\n",
    "\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "SEED = 1234\n",
    "\n",
    "# Load datasets correctly\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode=\"binary\", seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode=\"binary\", seed=SEED\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode=\"binary\", seed=SEED\n",
    ")\n",
    "\n",
    "# Define normalization layer\n",
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "# Apply normalization before batching\n",
    "def preprocess_ds(dataset):\n",
    "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)  # Avoids bottlenecks\n",
    "\n",
    "# Apply preprocessing\n",
    "train_ds = preprocess_ds(train_ds)\n",
    "val_ds = preprocess_ds(val_ds)\n",
    "test_ds = preprocess_ds(test_ds)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "HYPERPARAMS = {\n",
    "    \"lr\": [0.0001, 0.001, 0.01],\n",
    "    \"bs\": [32, 64],\n",
    "    \"drop\": [0.2, 0.5],\n",
    "    \"filters\": [32, 64],\n",
    "    \"kernel\": [(3, 3), (5, 5)],\n",
    "    \"dense\": [128, 256],\n",
    "    \"aug\": [True, False],\n",
    "    \"act\": [\"relu\", \"leaky_relu\"],\n",
    "}\n",
    "\n",
    "EPOCHS = 10  \n",
    "INPUT_SHAPE = (128, 128, 3)\n",
    "\n",
    "# Generate all possible hyperparameter combinations\n",
    "all_combinations = list(itertools.product(*HYPERPARAMS.values()))\n",
    "N_RANDOM_TRIALS = min(10, len(all_combinations))  \n",
    "random_combinations = random.sample(all_combinations, N_RANDOM_TRIALS)\n",
    "\n",
    "# Function to create CNN model\n",
    "def create_cnn_model(lr, drop, filters, kernel, dense, act, aug=None):\n",
    "    \"\"\"Creates and compiles a CNN model with flexible hyperparameters.\"\"\"\n",
    "    inputs = keras.Input(shape=INPUT_SHAPE)\n",
    "\n",
    "    # Apply augmentation before convolution layers if enabled\n",
    "    if aug:\n",
    "        x = aug(inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = layers.Conv2D(filters, kernel, activation=act)(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(filters * 2, kernel, activation=act)(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(filters * 4, kernel, activation=act)(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    # Flatten and fully connected layers\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(drop)(x)\n",
    "    x = layers.Dense(dense, activation=act)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Directory to save models\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_model_info = {}\n",
    "results = []  # Store results for all hyperparameter combinations\n",
    "\n",
    "# Training loop\n",
    "for i, hyperparams in enumerate(random_combinations, start=1):\n",
    "    print(f\"\\nTraining {i}/{N_RANDOM_TRIALS}: {hyperparams}...\")\n",
    "\n",
    "    # Unpack hyperparameters\n",
    "    lr, bs, drop, filters, kernel, dense, aug, act = hyperparams\n",
    "\n",
    "    # Fix LeakyReLU issue\n",
    "    if act == \"leaky_relu\":\n",
    "        act = layers.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "    # Define data augmentation if enabled\n",
    "    data_augmentation = (\n",
    "        keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.1),\n",
    "            layers.Resizing(128, 128)  # Ensures augmentation does not change image size\n",
    "        ])\n",
    "        if aug else None\n",
    "    )\n",
    "\n",
    "    # Create the CNN model\n",
    "    model = create_cnn_model(lr, drop, filters, kernel, dense, act, data_augmentation)\n",
    "\n",
    "    # Apply augmentation correctly\n",
    "    if aug:\n",
    "        train_ds_aug = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "    else:\n",
    "        train_ds_aug = train_ds\n",
    "\n",
    "    # **EARLY STOPPING CALLBACK**\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with no logs (verbose=0)\n",
    "    history = model.fit(\n",
    "        train_ds_aug,  \n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0  # Hide training logs\n",
    "    )\n",
    "\n",
    "    # Find the epoch where early stopping triggered\n",
    "    stopped_epoch = len(history.history[\"accuracy\"])\n",
    "\n",
    "    print(f\"  Early stopping triggered at epoch {stopped_epoch}.\")\n",
    "\n",
    "    # Evaluate on test dataset\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "    # Store results for this combination\n",
    "    model_info = {\n",
    "        \"Combination\": i,\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Batch Size\": bs,\n",
    "        \"Dropout\": drop,\n",
    "        \"Filters\": filters,\n",
    "        \"Kernel\": kernel,\n",
    "        \"Dense\": dense,\n",
    "        \"Augmentation\": aug,\n",
    "        \"Activation\": str(act),\n",
    "        \"Train Accuracy\": history.history[\"accuracy\"][-1],\n",
    "        \"Val Accuracy\": history.history[\"val_accuracy\"][-1],\n",
    "        \"Test Accuracy\": test_acc,\n",
    "    }\n",
    "    results.append(model_info)\n",
    "\n",
    "    # Track the best model\n",
    "    if model_info[\"Val Accuracy\"] > best_val_acc:\n",
    "        best_val_acc = model_info[\"Val Accuracy\"]\n",
    "        best_model_info = model_info\n",
    "\n",
    "        # Save the best model\n",
    "        best_model_path = os.path.join(save_dir, \"best_model.keras\")\n",
    "        model.save(best_model_path)\n",
    "\n",
    "        # Save training history\n",
    "        best_history_path = os.path.join(save_dir, \"best_history.json\")\n",
    "        with open(best_history_path, \"w\") as f:\n",
    "            json.dump(history.history, f)\n",
    "\n",
    "        print(f\"  New best model saved: {best_model_path}\")\n",
    "\n",
    "# Save all results\n",
    "results_path = os.path.join(save_dir, \"all_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Save best model info\n",
    "best_info_path = os.path.join(save_dir, \"best_model_info.json\")\n",
    "with open(best_info_path, \"w\") as f:\n",
    "    json.dump(best_model_info, f)\n",
    "\n",
    "print(f\"\\nAll results saved to: {results_path}\")\n",
    "print(f\"Best model details saved to: {best_info_path}\")\n",
    "print(f\"Best model file: {best_model_path}\")\n",
    "print(f\"Best training history file: {best_history_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3655844,
     "sourceId": 7863205,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22630.118305,
   "end_time": "2025-03-11T05:41:22.089511",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-10T23:24:11.971206",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
