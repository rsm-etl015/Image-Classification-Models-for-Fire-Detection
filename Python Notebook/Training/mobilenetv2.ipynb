{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b6e96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T06:20:23.867128Z",
     "iopub.status.busy": "2025-03-11T06:20:23.866746Z",
     "iopub.status.idle": "2025-03-11T09:49:37.232833Z",
     "shell.execute_reply": "2025-03-11T09:49:37.230859Z"
    },
    "papermill": {
     "duration": 12553.374563,
     "end_time": "2025-03-11T09:49:37.237952",
     "exception": false,
     "start_time": "2025-03-11T06:20:23.863389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1887 files belonging to 2 classes.\n",
      "Found 402 files belonging to 2 classes.\n",
      "Found 410 files belonging to 2 classes.\n",
      "\n",
      "Training 1/10: (0.01, 16, 0.3, 512, False, True, 0.75, 'swish')...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.75_128_no_top.h5\n",
      "\u001b[1m5903360/5903360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "  Early stopping triggered at epoch 3.\n",
      "  New best model saved: saved_models/best_model.keras\n",
      "\n",
      "Training 2/10: (0.01, 64, 0.5, 128, True, False, 0.75, 'swish')...\n",
      "  Early stopping triggered at epoch 3.\n",
      "  New best model saved: saved_models/best_model.keras\n",
      "\n",
      "Training 3/10: (0.0001, 64, 0.5, 128, True, False, 1.0, 'swish')...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "  Early stopping triggered at epoch 10.\n",
      "\n",
      "Training 4/10: (0.0001, 16, 0.3, 128, True, False, 0.75, 'relu')...\n",
      "  Early stopping triggered at epoch 7.\n",
      "  New best model saved: saved_models/best_model.keras\n",
      "\n",
      "Training 5/10: (0.01, 16, 0.2, 512, False, True, 1.0, 'relu')...\n",
      "  Early stopping triggered at epoch 3.\n",
      "\n",
      "Training 6/10: (0.01, 64, 0.3, 512, True, False, 0.75, 'relu')...\n",
      "  Early stopping triggered at epoch 6.\n",
      "\n",
      "Training 7/10: (0.01, 16, 0.5, 256, True, True, 0.75, 'relu')...\n",
      "  Early stopping triggered at epoch 3.\n",
      "\n",
      "Training 8/10: (0.001, 16, 0.5, 128, True, False, 0.75, 'relu')...\n",
      "  Early stopping triggered at epoch 4.\n",
      "\n",
      "Training 9/10: (0.01, 16, 0.2, 512, True, False, 0.5, 'relu')...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.5_128_no_top.h5\n",
      "\u001b[1m3201480/3201480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "  Early stopping triggered at epoch 6.\n",
      "\n",
      "Training 10/10: (0.001, 32, 0.5, 256, False, True, 1.0, 'swish')...\n",
      "  Early stopping triggered at epoch 5.\n",
      "\n",
      "All results saved to: saved_models/all_results.json\n",
      "Best model details saved to: saved_models/best_model_info.json\n",
      "Best model file: saved_models/best_model.keras\n",
      "Best training history file: saved_models/best_history.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Define dataset paths\n",
    "base_dir = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset_2n_version\"\n",
    "train_dir = f\"{base_dir}/train\"\n",
    "val_dir = f\"{base_dir}/val\"\n",
    "test_dir = f\"{base_dir}/test\"\n",
    "\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 16\n",
    "SEED = 1234\n",
    "\n",
    "# Load datasets correctly\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode=\"binary\", seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode=\"binary\", seed=SEED\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode=\"binary\", seed=SEED\n",
    ")\n",
    "\n",
    "# Define normalization layer\n",
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "\n",
    "# Apply normalization before batching\n",
    "def preprocess_ds(dataset):\n",
    "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)  # Avoids bottlenecks\n",
    "\n",
    "# Apply preprocessing\n",
    "train_ds = preprocess_ds(train_ds)\n",
    "val_ds = preprocess_ds(val_ds)\n",
    "test_ds = preprocess_ds(test_ds)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "HYPERPARAMS = {\n",
    "    \"lr\": [0.0001, 0.001, 0.01],  # Learning rate\n",
    "    \"bs\": [16, 32, 64],  # Batch size\n",
    "    \"drop\": [0.2, 0.3, 0.5],  # Dropout rate\n",
    "    \"dense\": [128, 256, 512],  # Dense layer size\n",
    "    \"aug\": [True, False],  # Data augmentation on/off\n",
    "    \"base_trainable\": [True, False],  # Fine-tune MobileNetV2 or freeze it\n",
    "    \"alpha\": [0.5, 0.75, 1.0],  # Width scaling (similar to filters in CNN)\n",
    "    \"activation\": [\"relu\", \"swish\"],  # Activation function for dense layers\n",
    "}\n",
    "\n",
    "EPOCHS = 10  \n",
    "INPUT_SHAPE = (128, 128, 3)\n",
    "\n",
    "# Generate all possible hyperparameter combinations\n",
    "all_combinations = list(itertools.product(*HYPERPARAMS.values()))\n",
    "N_RANDOM_TRIALS = min(10, len(all_combinations))  \n",
    "random_combinations = random.sample(all_combinations, N_RANDOM_TRIALS)\n",
    "\n",
    "# Function to create a MobileNetV2 model\n",
    "def create_cnn_model(lr, drop, dense, aug, base_trainable, alpha, activation):\n",
    "    \"\"\"Creates and compiles a MobileNetV2-based model while keeping all hyperparameters.\"\"\"\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(128, 128, 3), include_top=False, weights=\"imagenet\", alpha=alpha\n",
    "    )\n",
    "    base_model.trainable = base_trainable  # Fine-tune or freeze the base model\n",
    "\n",
    "    inputs = keras.Input(shape=(128, 128, 3))\n",
    "    \n",
    "    # Apply augmentation before passing through MobileNetV2\n",
    "    x = aug(inputs) if aug else inputs\n",
    "\n",
    "    x = base_model(x, training=base_trainable)  # Fine-tuning setting\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # Converts feature maps into a single vector\n",
    "    x = layers.Dense(dense, activation=activation)(x)\n",
    "    x = layers.Dropout(drop)(x)  # Dropout for regularization\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  # Binary classification\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Directory to save models\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_model_info = {}\n",
    "results = []  # Store results for all hyperparameter combinations\n",
    "\n",
    "# Training loop\n",
    "for i, hyperparams in enumerate(random_combinations, start=1):\n",
    "    print(f\"\\nTraining {i}/{N_RANDOM_TRIALS}: {hyperparams}...\")\n",
    "\n",
    "    # Unpack hyperparameters\n",
    "    lr, bs, drop, dense, aug, base_trainable, alpha, activation = hyperparams\n",
    "\n",
    "    # Define data augmentation if enabled\n",
    "    data_augmentation = (\n",
    "        keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.1),\n",
    "            layers.Resizing(128, 128)  # Ensures augmentation does not change image size\n",
    "        ])\n",
    "        if aug else None\n",
    "    )\n",
    "\n",
    "    # Create the MobileNetV2 model\n",
    "    model = create_cnn_model(lr, drop, dense, data_augmentation, base_trainable, alpha, activation)\n",
    "\n",
    "    # Apply augmentation correctly\n",
    "    if aug:\n",
    "        train_ds_aug = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "    else:\n",
    "        train_ds_aug = train_ds\n",
    "\n",
    "    # **EARLY STOPPING CALLBACK**\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with no logs (verbose=0)\n",
    "    history = model.fit(\n",
    "        train_ds_aug,  \n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0  # Hide training logs\n",
    "    )\n",
    "\n",
    "    # Find the epoch where early stopping triggered\n",
    "    stopped_epoch = len(history.history[\"accuracy\"])\n",
    "\n",
    "    print(f\"  Early stopping triggered at epoch {stopped_epoch}.\")\n",
    "\n",
    "    # Evaluate on test dataset\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "    # Store results (kept the same as before)\n",
    "    model_info = {\n",
    "        \"Combination\": i,\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Batch Size\": bs,\n",
    "        \"Dropout\": drop,\n",
    "        \"Dense\": dense,\n",
    "        \"Augmentation\": aug,\n",
    "        \"Base Model Trainable\": base_trainable,\n",
    "        \"Alpha\": alpha,\n",
    "        \"Activation\": activation,\n",
    "        \"Train Accuracy\": history.history[\"accuracy\"][-1],\n",
    "        \"Val Accuracy\": history.history[\"val_accuracy\"][-1],\n",
    "        \"Test Accuracy\": test_acc,\n",
    "    }\n",
    "    results.append(model_info)\n",
    "\n",
    "    # Track the best model\n",
    "    if model_info[\"Val Accuracy\"] > best_val_acc:\n",
    "        best_val_acc = model_info[\"Val Accuracy\"]\n",
    "        best_model_info = model_info\n",
    "\n",
    "        # Save the best model (same method as before)\n",
    "        best_model_path = os.path.join(save_dir, \"best_model.keras\")\n",
    "        model.save(best_model_path)\n",
    "\n",
    "        # Save training history (same method as before)\n",
    "        best_history_path = os.path.join(save_dir, \"best_history.json\")\n",
    "        with open(best_history_path, \"w\") as f:\n",
    "            json.dump(history.history, f)\n",
    "\n",
    "        print(f\"  New best model saved: {best_model_path}\")\n",
    "\n",
    "# Save all results (same method as before)\n",
    "results_path = os.path.join(save_dir, \"all_results.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "# Save best model info (same method as before)\n",
    "best_info_path = os.path.join(save_dir, \"best_model_info.json\")\n",
    "with open(best_info_path, \"w\") as f:\n",
    "    json.dump(best_model_info, f)\n",
    "\n",
    "print(f\"\\nAll results saved to: {results_path}\")\n",
    "print(f\"Best model details saved to: {best_info_path}\")\n",
    "print(f\"Best model file: {best_model_path}\")\n",
    "print(f\"Best training history file: {best_history_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3655844,
     "sourceId": 7863205,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12560.228622,
   "end_time": "2025-03-11T09:49:41.295998",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-11T06:20:21.067376",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
